{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -qqq torch --progress-bar off\n",
        "%pip install -qqq transformers --progress-bar off\n",
        "%pip install -qqq datasets --progress-bar off\n",
        "%pip install -qqq accelerate==0.34.2--progress-bar off\n",
        "%pip install -qqq bitsandbytes --progress-bar off\n",
        "%pip install -qqq peft --progress-bar off\n",
        "%pip install -qqq trl --progress-bar off\n",
        "%pip install -qqq colored --progress-bar off\n",
        "%pip install -qqq huggingface_hub --progress-bar off\n",
        "%pip install -qqq seaborn --progress-bar off"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from textwrap import dedent\n",
        "from typing import Dict, List"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947531873
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.colors as colors\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from colored import Back, Fore, Style\n",
        "from datasets import Dataset, load_dataset\n",
        "from matplotlib.ticker import PercentFormatter"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947545680
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftModel,\n",
        "    TaskType,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947550249
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline,\n",
        ")\n",
        "from trl import DataCollatorForCompletionOnlyLM, SFTConfig, SFTTrainer"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947552429
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "COLORS = [\"#bae1ff\", \"#ffb3ba\", \"#ffdfba\", \"#ffffba\", \"#baffc9\"]\n",
        "\n",
        "sns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n",
        "sns.set_palette(sns.color_palette(COLORS))\n",
        "\n",
        "cmap = colors.LinearSegmentedColormap.from_list(\"custom_cmap\", COLORS[:2])\n",
        "\n",
        "MY_STYLE = {\n",
        "    \"figure.facecolor\": \"black\",\n",
        "    \"axes.facecolor\": \"black\",\n",
        "    \"axes.edgecolor\": \"white\",\n",
        "    \"axes.labelcolor\": \"white\",\n",
        "    \"axes.linewidth\": 0.5,\n",
        "    \"text.color\": \"white\",\n",
        "    \"xtick.color\": \"white\",\n",
        "    \"ytick.color\": \"white\",\n",
        "    \"grid.color\": \"gray\",\n",
        "    \"grid.linestyle\": \"--\",\n",
        "    \"grid.linewidth\": 0.5,\n",
        "    \"axes.grid\": True,\n",
        "    \"xtick.labelsize\": \"medium\",\n",
        "    \"ytick.labelsize\": \"medium\",\n",
        "    \"axes.titlesize\": \"large\",\n",
        "    \"axes.labelsize\": \"large\",\n",
        "    \"lines.color\": COLORS[0],\n",
        "    \"patch.edgecolor\": \"white\",\n",
        "}\n",
        "\n",
        "mpl.rcParams.update(MY_STYLE)\n",
        "\n",
        "SEED = 42"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947552813
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947553005
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(SEED)\n",
        "PAD_TOKEN = \"<|pad|>\"\n",
        "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "NEW_MODEL = \"Llama-3-8B-Finetuning\""
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947553184
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login, whoami\n",
        "\n",
        "# Paste your token here\n",
        "login_token = \"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\"\n",
        "\n",
        "# Login to Hugging Face\n",
        "login(login_token)\n",
        "\n",
        "user_info = whoami()\n",
        "print(f\"Logged in as: {user_info['name']}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /home/azureuser/.cache/huggingface/token\nLogin successful\nLogged in as: priyadharshiniResolve\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947553613
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=quantization_config,\n",
        "    #     attn_implementation=\"flash_attention_2\",\n",
        "    #     attn_implementation=\"sdpa\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Loading checkpoint shards: 100%|██████████| 4/4 [02:20<00:00, 35.06s/it]\nThe new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\nThe new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "Embedding(128264, 4096)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947731198
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "LlamaConfig {\n  \"_attn_implementation_autoset\": true,\n  \"_name_or_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": false,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 128264\n}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947731423
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.bos_token, tokenizer.bos_token_id"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "('<|begin_of_text|>', 128000)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947731718
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "tokenizer.eos_token, tokenizer.eos_token_id"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "('<|eot_id|>', 128009)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947731978
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "tokenizer.pad_token, tokenizer.pad_token_id"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "('<|pad|>', 128256)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947732242
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "tokenizer.convert_tokens_to_ids(PAD_TOKEN)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "128256"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947732527
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict \n",
        "import pandas as pd"
      ],
      "outputs": [],
      "execution_count": 43,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733949552597
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Load SQuAD dataset\n",
        "dataset = load_dataset(\"squad\")\n",
        "\n",
        "# Split the original training dataset into 90% train and 10% validation\n",
        "train_valid_split = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "# Rename the split keys for clarity\n",
        "new_dataset = DatasetDict({\n",
        "    \"train\": train_valid_split[\"train\"],  # 90% of original train set\n",
        "    \"validation\": train_valid_split[\"test\"],  # 10% of original train set\n",
        "    \"test\": dataset[\"validation\"]  # Original validation set becomes test set\n",
        "}) \n",
        "\n",
        "# %%\n",
        "new_dataset['train']"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 44,
          "data": {
            "text/plain": "Dataset({\n    features: ['id', 'title', 'context', 'question', 'answers'],\n    num_rows: 78839\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733949555823
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = new_dataset['train'].to_pandas()\n",
        "df_train = pd.DataFrame(df_train)\n",
        "df_train['answers'] = df_train['answers'].apply(lambda x: x['text'][0] if isinstance(x, dict) and 'text' in x else x)\n",
        "df_train = df_train.drop(columns=['id','title'])\n"
      ],
      "outputs": [],
      "execution_count": 45,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733949560219
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid = new_dataset['validation'].to_pandas()\n",
        "df_valid = pd.DataFrame(df_valid)\n",
        "df_valid['answers'] = df_valid['answers'].apply(lambda x: x['text'][0] if isinstance(x, dict) and 'text' in x else x)\n",
        "df_valid = df_valid.drop(columns=['id','title'])\n"
      ],
      "outputs": [],
      "execution_count": 46,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733949565766
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = new_dataset['test'].to_pandas()\n",
        "df_test = pd.DataFrame(df_test)\n",
        "df_test['answers'] = df_test['answers'].apply(lambda x: x['text'][0] if isinstance(x, dict) and 'text' in x else x)\n",
        "df_test = df_test.drop(columns=['id','title'])\n"
      ],
      "outputs": [],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733949567566
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train dataset shape:\",df_train.shape)\n",
        "print(\"Valid dataset shape:\",df_valid.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "train dataset shape: (78839, 3)\nValid dataset shape: (8760, 3)\n"
        }
      ],
      "execution_count": 48,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733949577842
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.isnull().value_counts()\n",
        "\n",
        "# %%\n",
        "df_valid.isnull().value_counts()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 49,
          "data": {
            "text/plain": "context  question  answers\nFalse    False     False      8760\nName: count, dtype: int64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 49,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733949582300
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_example(row: dict):\n",
        "    prompt = dedent(\n",
        "        f\"\"\"\n",
        "    {row[\"question\"]}\n",
        "\n",
        "    Information:\n",
        "\n",
        "    ```\n",
        "    {row[\"context\"]}\n",
        "    ```\n",
        "    \"\"\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Use only the information to answer the question\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "        {\"role\": \"assistant\", \"content\": row[\"answers\"]},\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "df_train[\"text\"] = df_train.apply(format_example, axis=1)"
      ],
      "outputs": [],
      "execution_count": 50,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733949592129
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tokens(row: Dict) -> int:\n",
        "    return len(\n",
        "        tokenizer(\n",
        "            row[\"text\"],\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=False,\n",
        "        )[\"input_ids\"]\n",
        "    )\n",
        "df_train[\"token_count\"] = df_train.apply(count_tokens, axis=1)"
      ],
      "outputs": [],
      "execution_count": 51,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733949627180
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_example(row: dict):\n",
        "    prompt = dedent(\n",
        "        f\"\"\"\n",
        "    {row[\"question\"]}\n",
        "\n",
        "    Information:\n",
        "\n",
        "    ```\n",
        "    {row[\"context\"]}\n",
        "    ```\n",
        "    \"\"\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Use only the information to answer the question\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "        {\"role\": \"assistant\", \"content\": row[\"answers\"]},\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "df_valid[\"text\"] = df_valid.apply(format_example, axis=1)"
      ],
      "outputs": [],
      "execution_count": 52,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733949627579
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tokens(row: Dict) -> int:\n",
        "    return len(\n",
        "        tokenizer(\n",
        "            row[\"text\"],\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=False,\n",
        "        )[\"input_ids\"]\n",
        "    )\n",
        "df_valid[\"token_count\"] = df_valid.apply(count_tokens, axis=1)"
      ],
      "outputs": [],
      "execution_count": 53,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733949632847
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.to_json(\"train.json\", orient=\"records\", lines=True)\n",
        "df_valid.to_json(\"val.json\", orient=\"records\", lines=True)\n",
        "\n",
        "# %%\n",
        "df_test.to_json(\"test.json\", orient=\"records\", lines=True)"
      ],
      "outputs": [],
      "execution_count": 54,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733949638164
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files={\"test\": \"test.json\"},\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Generating test split: 10570 examples [00:00, 108718.02 examples/s]\n"
        }
      ],
      "execution_count": 55,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733949641321
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files={\"train\": \"train.json\", \"validation\": \"val.json\"},\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Generating train split: 78839 examples [00:01, 49001.67 examples/s]\nGenerating validation split: 8760 examples [00:00, 48090.06 examples/s]\n"
        }
      ],
      "execution_count": 56,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733949646189
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset\n",
        "\n",
        "# %%\n",
        "print(dataset[\"train\"][0][\"text\"])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\nUse only the information to answer the question<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nTo show their strength in the international Communist movement, what did China do?\n\nInformation:\n\n```\nAfter the formation of the People's Republic of China in 1949, the Chinese government named the Western nations, led by the United States, as the biggest threat to its national security. Basing this judgment on China's century of humiliation beginning in the early 19th century, American support for the Nationalists during the Chinese Civil War, and the ideological struggles between revolutionaries and reactionaries, the Chinese leadership believed that China would become a critical battleground in the United States' crusade against Communism. As a countermeasure and to elevate China's standing among the worldwide Communist movements, the Chinese leadership adopted a foreign policy that actively promoted Communist revolutions throughout territories on China's periphery.\n```<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\npromoted Communist revolutions<|eot_id|>\n"
        }
      ],
      "execution_count": 57,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733949649272
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128,\n",
        "    return_full_text=False,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 58,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733949652213
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_test_prompt(data_row):\n",
        "    prompt = dedent(\n",
        "        f\"\"\"\n",
        "    {data_row[\"question\"]}\n",
        "\n",
        "    Information:\n",
        "\n",
        "    ```\n",
        "    {data_row[\"context\"]}\n",
        "    ```\n",
        "    \"\"\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Use only the information to answer the question\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "# %%\n",
        "row = dataset[\"validation\"][0]\n",
        "prompt = create_test_prompt(row)\n",
        "print(prompt)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\nUse only the information to answer the question<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nWhat percentage of Egyptians polled support death penalty for those leaving Islam?\n\nInformation:\n\n```\nThe Pew Forum on Religion & Public Life ranks Egypt as the fifth worst country in the world for religious freedom. The United States Commission on International Religious Freedom, a bipartisan independent agency of the US government, has placed Egypt on its watch list of countries that require close monitoring due to the nature and extent of violations of religious freedom engaged in or tolerated by the government. According to a 2010 Pew Global Attitudes survey, 84% of Egyptians polled supported the death penalty for those who leave Islam; 77% supported whippings and cutting off of hands for theft and robbery; and 82% support stoning a person who commits adultery.\n```<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n"
        }
      ],
      "execution_count": 59,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733949654271
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "outputs = pipe(prompt)\n",
        "response = f\"\"\"\n",
        "answer:     {row[\"answers\"]}\n",
        "prediction: {outputs[0][\"generated_text\"]}\n",
        "\"\"\"\n",
        "print(response)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nanswer:     84%\nprediction: 84% of Egyptians polled supported the death penalty for those who leave Islam.\n\nCPU times: user 1.85 s, sys: 356 ms, total: 2.21 s\nWall time: 4.37 s\n"
        }
      ],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row = dataset[\"validation\"][1]\n",
        "prompt = create_test_prompt(row)\n",
        "print(prompt)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\nUse only the information to answer the question<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nAnn Arbor ranks 1st among what goods sold?\n\nInformation:\n\n```\nThe Ann Arbor Hands-On Museum is located in a renovated and expanded historic downtown fire station. Multiple art galleries exist in the city, notably in the downtown area and around the University of Michigan campus. Aside from a large restaurant scene in the Main Street, South State Street, and South University Avenue areas, Ann Arbor ranks first among U.S. cities in the number of booksellers and books sold per capita. The Ann Arbor District Library maintains four branch outlets in addition to its main downtown building. The city is also home to the Gerald R. Ford Presidential Library.\n```<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n"
        }
      ],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733945406677
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "outputs = pipe(prompt)\n",
        "response = f\"\"\"\n",
        "answer:     {row[\"answers\"]}\n",
        "prediction: {outputs[0][\"generated_text\"]}\n",
        "\"\"\"\n",
        "print(response)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nanswer:     books\nprediction: Ann Arbor ranks 1st among U.S. cities in the number of booksellers and books sold per capita.\n\nCPU times: user 1.3 s, sys: 16.2 ms, total: 1.31 s\nWall time: 1.31 s\n"
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "for row in tqdm(dataset[\"validation\"]):\n",
        "    prompt = create_test_prompt(row)\n",
        "    outputs = pipe(prompt)\n",
        "    rows.append(\n",
        "        {\n",
        "            \"question\": row[\"question\"],\n",
        "            \"context\": row[\"context\"],\n",
        "            \"prompt\": prompt,\n",
        "            \"answer\": row[\"answer\"],\n",
        "            \"untrained_prediction\": outputs[0][\"generated_text\"],\n",
        "        }\n",
        "    )\n",
        "\n",
        "predictions_df = pd.DataFrame(rows)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "  0%|          | 0/8760 [00:00<?, ?it/s]\n"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'answer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[36], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m create_test_prompt(row)\n\u001b[1;32m      4\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m pipe(prompt)\n\u001b[1;32m      5\u001b[0m     rows\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m      6\u001b[0m         {\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[0;32m---> 10\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muntrained_prediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     12\u001b[0m         }\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     15\u001b[0m predictions_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'answer'"
          ]
        }
      ],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733945414909
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer)\n",
        "\n",
        "# %%\n",
        "response_template = \"<|end_header_id|>\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
        "\n",
        "examples = [dataset[\"train\"][0][\"text\"]]\n",
        "encodings = [tokenizer(e) for e in examples]\n",
        "\n",
        "dataloader = DataLoader(encodings, collate_fn=collator, batch_size=1)"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733945442513
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(dataloader))\n",
        "batch.keys()\n",
        "\n",
        "# %%\n",
        "batch[\"labels\"]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,    271,  25475,   9437,  37961,  93574, 128009]])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 38,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733945444602
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model\n",
        "\n",
        "# %%\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\n",
        "        \"self_attn.q_proj\",\n",
        "        \"self_attn.k_proj\",\n",
        "        \"self_attn.v_proj\",\n",
        "        \"self_attn.o_proj\",\n",
        "        \"mlp.gate_proj\",\n",
        "        \"mlp.up_proj\",\n",
        "        \"mlp.down_proj\",\n",
        "    ],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# %%\n",
        "model.print_trainable_parameters()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "trainable params: 83,886,080 || all params: 8,114,212,864 || trainable%: 1.0338\n"
        }
      ],
      "execution_count": 39,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733945448836
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = \"experiments\"\n",
        "\n",
        "# %%\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=0.2,\n",
        "    save_steps=0.2,\n",
        "    logging_steps=10,\n",
        "    learning_rate=1e-4,\n",
        "    fp16=True,  # or bf16=True,\n",
        "    save_strategy=\"steps\",\n",
        "    warmup_ratio=0.1,\n",
        "    save_total_limit=2,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"none\",\n",
        "    save_safetensors=True,\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False,  # We template with special tokens\n",
        "        \"append_concat_token\": False,  # No need to add additional separator token\n",
        "    },\n",
        "    seed=SEED,\n",
        ")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 59,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947259112
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_config,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": 60,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947262479
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "outputs": [],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733946603177
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Wed Dec 11 19:50:04 2024       \r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 535.216.01             Driver Version: 535.216.01   CUDA Version: 12.2     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  NVIDIA A100 80GB PCIe          On  | 00000001:00:00.0 Off |                    0 |\r\n| N/A   34C    P0              74W / 300W |    513MiB / 81920MiB |      0%      Default |\r\n|                                         |                      |             Disabled |\r\n+-----------------------------------------+----------------------+----------------------+\r\n|   1  NVIDIA A100 80GB PCIe          On  | 00000002:00:00.0 Off |                    0 |\r\n| N/A   36C    P0              78W / 300W |  31337MiB / 81920MiB |      0%      Default |\r\n|                                         |                      |             Disabled |\r\n+-----------------------------------------+----------------------+----------------------+\r\n|   2  NVIDIA A100 80GB PCIe          On  | 00000003:00:00.0 Off |                    0 |\r\n| N/A   38C    P0              76W / 300W |  33111MiB / 81920MiB |      0%      Default |\r\n|                                         |                      |             Disabled |\r\n+-----------------------------------------+----------------------+----------------------+\r\n|   3  NVIDIA A100 80GB PCIe          On  | 00000004:00:00.0 Off |                    0 |\r\n| N/A   37C    P0              72W / 300W |  72327MiB / 81920MiB |      0%      Default |\r\n|                                         |                      |             Disabled |\r\n+-----------------------------------------+----------------------+----------------------+\r\n                                                                                         \r\n+---------------------------------------------------------------------------------------+\r\n| Processes:                                                                            |\r\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n|        ID   ID                                                             Usage      |\r\n|=======================================================================================|\r\n|    0   N/A  N/A     25655      C   ...envs/azureml_py310_sdkv2/bin/python      504MiB |\r\n|    1   N/A  N/A     25655      C   ...envs/azureml_py310_sdkv2/bin/python    31328MiB |\r\n|    2   N/A  N/A     25655      C   ...envs/azureml_py310_sdkv2/bin/python    33102MiB |\r\n|    3   N/A  N/A     25655      C   ...envs/azureml_py310_sdkv2/bin/python    72318MiB |\r\n+---------------------------------------------------------------------------------------+\r\n"
        }
      ],
      "execution_count": 48,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ],
      "outputs": [],
      "execution_count": 50,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733946805805
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()"
      ],
      "outputs": [],
      "execution_count": 57,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947221816
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if last_checkpoint:\n",
        "#     print(f\"Resuming training from {last_checkpoint}...\")\n",
        "#     trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "# else:\n",
        "#     print(\"No checkpoint found; starting fresh training...\")\n",
        "trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cuda:1!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[62], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# if last_checkpoint:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     print(f\"Resuming training from {last_checkpoint}...\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     trainer.train(resume_from_checkpoint=last_checkpoint)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     print(\"No checkpoint found; starting fresh training...\")\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/accelerate/utils/operations.py:823\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/accelerate/utils/operations.py:811\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/accelerate/utils/operations.py:823\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/accelerate/utils/operations.py:811\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "    \u001b[0;31m[... skipping similar frames: ConvertOutputsToFp32.__call__ at line 811 (4 times), autocast_decorator.<locals>.decorate_autocast at line 44 (4 times), convert_outputs_to_fp32.<locals>.forward at line 823 (4 times)]\u001b[0m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/accelerate/utils/operations.py:823\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/accelerate/utils/operations.py:811\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/peft/peft_model.py:1644\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1643\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1655\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1214\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1212\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1214\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1217\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/loss/loss_utils.py:46\u001b[0m, in \u001b[0;36mForCausalLMLoss\u001b[0;34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Enable model parallelism\u001b[39;00m\n\u001b[1;32m     45\u001b[0m shift_labels \u001b[38;5;241m=\u001b[39m shift_labels\u001b[38;5;241m.\u001b[39mto(shift_logits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 46\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mfixed_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshift_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/loss/loss_utils.py:28\u001b[0m, in \u001b[0;36mfixed_cross_entropy\u001b[0;34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(source, target, ignore_index\u001b[38;5;241m=\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39mreduction)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cuda:1!"
          ]
        }
      ],
      "execution_count": 62,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733947271710
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if checkpoint exists\n",
        "checkpoint_path = \"experiments/checkpoint-9855\"\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(\"Checkpoint found. Loading checkpoint...\")\n",
        "    trainer.train(resume_from_checkpoint=checkpoint_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_MODEL_DIR = \"CUSTOM-MODEL\"\n",
        "# Save Model\n",
        "trainer.save_model(SAVE_MODEL_DIR)\n",
        "\n",
        "# Explicitly ensure config is saved\n",
        "model.config.save_pretrained(SAVE_MODEL_DIR)\n",
        "\n",
        "# %%\n",
        "tokenizer.save_pretrained(NEW_MODEL)\n",
        "\n",
        "# %%\n",
        "print(type(model))  # Should be the model class (e.g., AutoModelForSequenceClassification)\n",
        "print(type(trainer))\n",
        "print(hasattr(model, \"train\"))  # This should return True\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(NEW_MODEL, tokenizer=tokenizer, max_shard_size=\"5GB\")\n",
        "\n",
        "# %%\n",
        "tokenizer.push_to_hub(NEW_MODEL)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files={\"train\": \"train.json\", \"validation\": \"val.json\"},\n",
        ")\n",
        "dataset"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NEW_MODEL = \"CUSTOM-MODEL\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(NEW_MODEL)\n",
        "print(\"tokenizer loaded\")\n",
        "print(\"tokenizer length:\",len(tokenizer))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, quantization_config=quantization_config, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "\n",
        "# %%\n",
        "len(tokenizer)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "SAVED_MODEL = \"/home/azureuser/cloudfiles/code/Llama-3-8B-Instruct-SQUAD-RAG\"\n",
        "CHECKPOINT_9855 = \"/home/azureuser/cloudfiles/code/experiments/checkpoint-9855\"\n",
        "CHECKPOINT_7884 = \"/home/azureuser/cloudfiles/code/experiments/checkpoint-7884\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733944095468
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_saved_model = AutoConfig.from_pretrained(CHECKPOINT_7884)\n",
        "print(f\"Vocabulary size in the saved model: {config_saved_model.vocab_size}\")\n",
        "\n",
        "# %%\n",
        "len(tokenizer)\n",
        "\n",
        "# %%\n",
        "# Resize the model embeddings to match the tokenizer length\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# %%\n",
        "config_checkpoint_model = AutoConfig.from_pretrained(CHECKPOINT_9855)\n",
        "print(f\"Vocabulary size in the checkpoint model: {config_checkpoint_model.vocab_size}\")\n",
        "\n",
        "# %%\n",
        "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)\n",
        "\n",
        "# %%\n",
        "print(new_vocab_size)\n",
        "print(\"Model Name:\",MODEL_NAME)\n",
        "\n",
        "# %%\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128,\n",
        "    return_full_text=False,\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_test_prompt(data_row):\n",
        "    prompt = dedent(\n",
        "        f\"\"\"\n",
        "    {data_row[\"question\"]}\n",
        "\n",
        "    Information:\n",
        "\n",
        "    ```\n",
        "    {data_row[\"context\"]}\n",
        "    ```\n",
        "    \"\"\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Use only the information to answer the question\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "# %%\n",
        "row = test_dataset[\"test\"][0]\n",
        "prompt = create_test_prompt(row)\n",
        "print(prompt)\n",
        "\n",
        "# %%\n",
        "def format_example(row: dict):\n",
        "    prompt = dedent(\n",
        "        f\"\"\"\n",
        "    {row[\"question\"]}\n",
        "\n",
        "    Information:\n",
        "\n",
        "    ```\n",
        "    {row[\"context\"]}\n",
        "    ```\n",
        "    \"\"\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Use only the information to answer the question\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "        {\"role\": \"assistant\", \"content\": row[\"answers\"]},\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "df_test[\"text\"] = df_test.apply(format_example, axis=1)\n",
        "\n",
        "def count_tokens(row: Dict) -> int:\n",
        "    return len(\n",
        "        tokenizer(\n",
        "            row[\"text\"],\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=False,\n",
        "        )[\"input_ids\"]\n",
        "    )\n",
        "df_test[\"token_count\"] = df_test.apply(count_tokens, axis=1)\n",
        "\n",
        "# %%\n",
        "%%time\n",
        "outputs = pipe(prompt)\n",
        "print(\"outputs:\",outputs)\n",
        "response = f\"\"\"\n",
        "answer:     {row[\"answers\"]}\n",
        "prediction: {outputs[0][\"generated_text\"]}\n",
        "\"\"\"\n",
        "print(response)\n",
        "\n",
        "# %%\n",
        "%%time\n",
        "outputs = pipe(prompt)\n",
        "response = f\"\"\"\n",
        "answer:     {row[\"answer\"]}\n",
        "prediction: {outputs[0][\"generated_text\"]}\n",
        "\"\"\"\n",
        "print(response)\n",
        "\n",
        "# %%\n",
        "from accelerate import Accelerator\n",
        "import torch\n",
        "\n",
        "# Function to process data using accelerate\n",
        "def process_data_on_gpus(data):\n",
        "    # Initialize the accelerator to handle multi-GPU setup\n",
        "    accelerator = Accelerator()\n",
        "    data = accelerator.gather(data)\n",
        "    prompts = [create_test_prompt(row) for row in data]\n",
        "    return prompts\n",
        "\n",
        "# Use the accelerator to run across multiple GPUs\n",
        "prompts = process_data_on_gpus(test_dataset[\"test\"])\n",
        "\n",
        "# Print the first 10 processed prompts (as an example)\n",
        "for prompt in prompts[:10]:\n",
        "    print(prompt)\n",
        "\n",
        "\n",
        "# %%\n",
        "row = test_dataset[\"test\"][1]\n",
        "prompt = create_test_prompt(row)\n",
        "print(prompt)\n",
        "\n",
        "# %%\n",
        "%pip install evaluate\n",
        "\n",
        "# %%\n",
        "len(prompts)\n",
        "\n",
        "# %%\n",
        "%pip install sacrebleu\n",
        "\n",
        "\n",
        "# %%\n",
        "%pip install nltk\n",
        "\n",
        "# %%\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# %%\n",
        "from accelerate import Accelerator\n",
        "import time\n",
        "import evaluate\n",
        "from tqdm import tqdm  # Importing tqdm for progress bar\n",
        "\n",
        "\n",
        "# Example function to process each prompt and calculate F1 score\n",
        "def generate_responses_and_calculate_f1(prompts, test_dataset, pipe):\n",
        "    accelerator = Accelerator()  # Initialize accelerator for multi-GPU handling\n",
        "    \n",
        "    responses = []\n",
        "    true_answers = []\n",
        "    predicted_answers = []\n",
        "\n",
        "    # Using tqdm to add a progress bar to the loop\n",
        "    for i, prompt in tqdm(enumerate(prompts[:5]), total=100, desc=\"Processing Prompts\"):\n",
        "        # Generate output using the model pipeline\n",
        "        outputs = pipe(prompt)\n",
        "\n",
        "        # Get the generated text (prediction) from the model output\n",
        "        prediction = outputs[0][\"generated_text\"]\n",
        "\n",
        "        # Get the ground truth answer\n",
        "        answer = test_dataset[\"test\"][i][\"answers\"]\n",
        "        # Store the true answer and predicted answer\n",
        "        true_answers.append(answer)\n",
        "        predicted_answers.append(prediction)\n",
        "\n",
        "    print(true_answers)\n",
        "    print(predicted_answers)\n",
        "\n",
        "# Run the function and calculate F1 score\n",
        "generate_responses_and_calculate_f1(prompts, test_dataset, pipe)\n",
        "\n",
        "\n",
        "# %%\n",
        "predicted_answers = ['Denver Broncos', 'Carolina Panthers', 'Santa Clara, California', 'Denver Broncos', 'gold']\n",
        "\n",
        "true_answers = ['Denver Broncos', 'Carolina Panthers', \"Levi's Stadium\", 'Denver Broncos', 'gold']\n",
        "\n",
        "# %%\n",
        "# Compute BLEU score\n",
        "bleu_score = sentence_bleu([true_answers],predicted_answers)\n",
        "print(\"BLEU Score:\", bleu_score)\n",
        "\n",
        "# %%\n",
        "# Print the first 5 responses and the F1 score\n",
        "for response in responses[:5]:\n",
        "    print(response)\n",
        "\n",
        "\n",
        "# %%\n",
        "from accelerate import Accelerator\n",
        "import time\n",
        "\n",
        "# Assuming 'pipe' is your pipeline and 'test_dataset' contains the rows with answers.\n",
        "# Example function to process each prompt with the model pipeline and generate response\n",
        "def generate_responses(prompts, test_dataset, pipe):\n",
        "    accelerator = Accelerator()  # Initialize accelerator for GPU distribution\n",
        "    \n",
        "    responses = []\n",
        "\n",
        "    # Process each prompt in parallel using Accelerator\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        # Simulate a model inference call using pipe(prompt)\n",
        "        # Assuming that 'pipe' processes the prompt and returns outputs\n",
        "        outputs = pipe(prompt)\n",
        "\n",
        "        # Format the response with the answer and the generated prediction\n",
        "        response = f\"\"\"\n",
        "        answer:     {test_dataset[\"test\"][i][\"answers\"]}\n",
        "        prediction: {outputs[0][\"generated_text\"]}\n",
        "        \"\"\"\n",
        "        \n",
        "        responses.append(response)\n",
        "\n",
        "    return responses\n",
        "\n",
        "# Example of gathering all prompts and running them through the pipeline\n",
        "prompts = [create_test_prompt(row) for row in test_dataset[\"test\"]]  # Create prompts for each row\n",
        "\n",
        "# Use %time to measure the execution time of the function\n",
        "start_time = time.time()\n",
        "\n",
        "# Run the model inference across all prompts and generate responses\n",
        "responses = generate_responses(prompts, test_dataset, pipe)\n",
        "\n",
        "# Print the first 5 responses as an example\n",
        "for response in responses[:5]:\n",
        "    print(response)\n",
        "\n",
        "# Measure and print the execution time\n",
        "end_time = time.time()\n",
        "print(f\"Processing time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "# %%\n",
        "%%time\n",
        "outputs = pipe(prompt)\n",
        "response = f\"\"\"\n",
        "answer:     {row[\"answers\"]}\n",
        "prediction: {outputs[0][\"generated_text\"]}\n",
        "\"\"\"\n",
        "print(response)\n",
        "\n",
        "# %%\n",
        "row = dataset[\"test\"][2]\n",
        "prompt = create_test_prompt(row)\n",
        "print(prompt)\n",
        "\n",
        "# %%\n",
        "%%time\n",
        "outputs = pipe(prompt)\n",
        "response = f\"\"\"\n",
        "answer:     {row[\"answer\"]}\n",
        "prediction: {outputs[0][\"generated_text\"]}\n",
        "\"\"\"\n",
        "print(response)\n",
        "\n",
        "# %%\n",
        "predictions = []\n",
        "for row in tqdm(dataset[\"test\"]):\n",
        "    outputs = pipe(create_test_prompt(row))\n",
        "    predictions.append(outputs[0][\"generated_text\"])\n",
        "\n",
        "# %%\n",
        "predictions_df.head()\n",
        "\n",
        "# %%\n",
        "predictions_df[\"trained_prediction\"] = predictions\n",
        "\n",
        "# %%\n",
        "predictions_df.head()\n",
        "\n",
        "# %%\n",
        "predictions_df.to_csv(\"predictions.csv\", index=None)\n",
        "\n",
        "# %%\n",
        "sample = predictions_df.sample(n=20)\n",
        "sample.head()\n",
        "\n",
        "# %%\n",
        "for i, row in sample.head(n=10).reset_index().iterrows():\n",
        "    print(f\"{Fore.DARK_VIOLET_1A}{Back.WHITE}Example {i + 1}{Style.reset}\")\n",
        "    response = f\"\"\"\n",
        "{Fore.BLUE}answer:{Style.reset} {row['answer']}\n",
        "\n",
        "{Fore.GREEN}trained:{Style.reset} {row['trained_prediction']}\n",
        "\n",
        "{Fore.DARK_ORANGE}untrained:{Style.reset} {row['untrained_prediction']}\n",
        "\"\"\"\n",
        "    print(response)\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}